{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import xmltodict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import torch\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPU:  1\n",
      "GPU Name:  NVIDIA GeForce RTX 4060\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of GPU: \", torch.cuda.device_count())\n",
    "print(\"GPU Name: \", torch.cuda.get_device_name())\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 4 files: 100%|██████████| 4/4 [00:00<?, ?it/s]\n",
      "c:\\Users\\pilug\\Documents\\PaperSemanticSearch\\.venv\\lib\\site-packages\\adapters\\loading.py:165: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(weights_file, map_location=\"cpu\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'specter2'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Install necessary libraries if not already installed:\n",
    "# pip install transformers\n",
    "# pip install adapter-transformers\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from adapters import AutoAdapterModel\n",
    "import torch  # Required for handling tensors\n",
    "\n",
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('allenai/specter2_base')\n",
    "model = AutoAdapterModel.from_pretrained('allenai/specter2_base')\n",
    "\n",
    "# Load the adapter for Specter2\n",
    "model.load_adapter(\"allenai/specter2\", source=\"hf\", load_as=\"specter2\", set_active=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2664"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json('papers.json', orient='records', lines=True)\n",
    "df = df.drop_duplicates(subset=['title'], keep='first')\n",
    "df = df[df['summary'].str.strip() != '']\n",
    "df = df.reset_index(drop=True)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding Papers: 49it [00:15,  2.77it/s]C:\\Users\\pilug\\AppData\\Local\\Temp\\ipykernel_2092\\2511239699.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  existing_embeddings = torch.load(embedding_file)\n",
      "Embedding Papers: 2664it [12:52,  3.45it/s]\n",
      "C:\\Users\\pilug\\AppData\\Local\\Temp\\ipykernel_2092\\2511239699.py:54: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  existing_embeddings = torch.load(embedding_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final total embeddings saved: 2755\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "\n",
    "# Function to handle long texts with chunking\n",
    "def embed_long_text(title, abstract, tokenizer, model, max_length=512):\n",
    "    chunk_size = max_length - 10 \n",
    "    chunks = [abstract[i:i + chunk_size] for i in range(0, len(abstract), chunk_size)]\n",
    "\n",
    "    # Create tokenized inputs for each chunk\n",
    "    embeddings = []\n",
    "    for chunk in chunks:\n",
    "        text = title + tokenizer.sep_token + chunk\n",
    "        inputs = tokenizer(\n",
    "            text,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "            return_token_type_ids=False,\n",
    "            max_length=max_length\n",
    "        )\n",
    "        with torch.no_grad():  # Avoid gradient calculations for inference\n",
    "            output = model(**inputs)\n",
    "        # Take the first token in the chunk as the embedding\n",
    "        embeddings.append(output.last_hidden_state[:, 0, :])\n",
    "\n",
    "    # Aggregate embeddings (e.g., average)\n",
    "    aggregated_embedding = torch.mean(torch.stack(embeddings), dim=0)\n",
    "    return aggregated_embedding\n",
    "\n",
    "batches = 50\n",
    "batch_embeddings = []\n",
    "\n",
    "embedding_file = \"embeddings.pt\"\n",
    "if not os.path.exists(embedding_file) or os.path.getsize(embedding_file) == 0:\n",
    "    torch.save([], embedding_file)  # Save an empty list initially\n",
    "\n",
    "for idx, paper in tqdm(df.iterrows(), desc='Embedding Papers'):\n",
    "    embedding = embed_long_text(\n",
    "        paper['title'],\n",
    "        paper['summary'],\n",
    "        tokenizer,\n",
    "        model\n",
    "    )\n",
    "    batch_embeddings.append(embedding)\n",
    "    if len(batch_embeddings) == batches:\n",
    "        existing_embeddings = torch.load(embedding_file)\n",
    "        existing_embeddings.extend(batch_embeddings)\n",
    "        print(f\"Processed {idx} and {len(existing_embeddings)} embeddings\")\n",
    "        torch.save(existing_embeddings, embedding_file)\n",
    "        batch_embeddings = []\n",
    "\n",
    "# Save any remaining embeddings in the batch\n",
    "if batch_embeddings:\n",
    "    existing_embeddings = torch.load(embedding_file)\n",
    "    existing_embeddings.extend(batch_embeddings)\n",
    "    torch.save(existing_embeddings, embedding_file)\n",
    "    print(f\"Final total embeddings saved: {len(existing_embeddings)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
